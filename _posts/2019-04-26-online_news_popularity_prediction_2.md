---
title: "Online News Popularirty Prediction - Part II"
date: 2019-04-26
tags: [Machine Learning, Data Pre-processing, Python]
header:
  overlay_image: /assets/190426/ytcount-online_news.jpg
  caption: "Photo credit: [**YTcount**](https://unsplash.com)"
  teaser: /assets/190426/ytcount-online_news.jpg
excerpt: "Using Python to perform data pre-processing and data modeling. (Difficulty: ★★☆☆☆)"
toc: true
toc_label: "Content"
toc_sticky: true
---
_Followed by [the last post](https://chw18019.github.io/online_news_popularity_prediction_1/), you can find how the analyses were conducted. In this post, I used [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true) to perform the same analysis with `Python`, including **data import, data pre-processing, and modeling**. For the complete jupyter notebook, you can find it on my GitHub repository [<i class="fab fa-fw fa-github" aria-hidden="true"></i>](https://github.com/chw18019/OnlineNewsPopularity/blob/master/TDI_ProjectProposal.ipynb)._
{: .notice--primary}

## Data Import
First of all, you need to mount your google drive, which means you authorize google colab to use your files in the drive.
```python
from google.colab import drive
drive.mount('/content/drive')
```
Then read the data into `Pandas` dataframe called "dataset".
```python
import pandas as pd
import numpy as np
dataset = pd.read_csv("/GOOGLE DRIVE PATH/FILE NAME.csv")
```

## Data Pre-processing
In the beginning, we delete the first two columns, which are URL and delta time (the time that the article was posted.)
```python
df0 = dataset.loc[:," n_tokens_title":" shares"]
```
### ✳︎ Missing Values Issue
There're lots of 0 value in the column "n_unique_tokens", and by the definition, we can view them as missing values. The "n_unique_tokens" represents the rates of unique words in an article. If it equals zero, it means the article doesn't have one word in it.
```python
# change the 0 into np.nan, and we can find 1,181 rows of missing values
df0[" n_unique_tokens"].replace(0, np.nan, inplace= True)
df0[" n_unique_tokens"].isnull().sum()

# drop all the missing values and called it df1
df1 = df0.dropna()

# check how many rows and columns in each dataset
print(df0.shape)
print(df1.shape)
```

### ✳︎ Outlier Issue
There're several methods you can find online when it comes to outlier detection. Here, I showed two of them to show you how it work in the function.
#### Method1 for a Normal Distribution
```python
anomalies = []
def find_anomalies(random_data):
    # Set upper and lower limit to 3 standard deviation
    random_data_std = np.std(random_data)
    random_data_mean = np.mean(random_data)
    anomaly_cut_off = random_data_std * 3
    
    lower_limit  = random_data_mean - anomaly_cut_off 
    upper_limit = random_data_mean + anomaly_cut_off
    print(lower_limit)
    # Generate outliers
    for outlier in random_data:
        if outlier > upper_limit or outlier < lower_limit:
            anomalies.append(outlier)
    return anomalies

find_anomalies(df1[' n_unique_tokens'])
```
This method would require a normality test. This part involves more statistics knowledge. Because I want to focus on machine learning and python code, I won't focus on this for now. If you'd like to, you can find more details in [this article](https://towardsdatascience.com/understanding-the-68-95-99-7-rule-for-a-normal-distribution-b7b7cbf760c2).

Now that assumes the data is a normal distribution, based on 68-95-99.7 rule, 99.7% of the data is within 3 standard deviations(σ) of the mean (μ).

{% include figure image_path="/assets/190426/68-95-99.png" alt="68-95-99" %}

#### Method2 using Boxplots
```python
def remove_outlier(df_in, col_name):
    q1 = df_in[col_name].quantile(0.25)
    q3 = df_in[col_name].quantile(0.75)
    
    #Interquartile range
    iqr = q3-q1 
    fence_low  = q1-1.5*iqr
    fence_high = q3+1.5*iqr
    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]
    return df_out


df2 = remove_outlier(df1,' n_unique_tokens')
```
Another way to detect outliers is by using boxplots. The image below is a boxplot. A simple explanation can be found in [Michael Galarnyk's Medium blog](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51). 

{% include figure image_path="/assets/190426/boxplot.png" alt="boxplot" %}
> A boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”).

Be careful, boxplot is not a guarantee to detect outlier. The more common way is to look into the distribution, and determine outliers on a case-by-case basis. Always keep an eye on the effect of outliers in your analyses.

For your information, a boxplot can be generated by using `seaborn` with only one line of code.
```python
import seaborn as sns

sns.boxplot(df2[" n_unique_tokens"])
```

### ✳︎ Convert dummy variables into a categorical data
Usually, categorical data is converted into dummy variables for modeling. But here, we did the opposite just in case some visualization tools or packages require a categorical format. 
```python
# subset the channel data based on column index
ch = df2.iloc[:,11:17]

# rename the columns
ch.rename(columns=lambda x:x[17:], inplace=True)

# convert them back to a categorical data
ch = ch.idxmax(axis=1)
```

## Data Visualization
Before modeling, it's essential to get familiar with the data. Nowadays, [Matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/) are the most commonly used. If you are interested in this particular part, I'll have another post to cover it. 

```python
# distribution plots for each columns
num = [f for f in df2.columns if df2.dtypes[f] != 'object']
nd = pd.melt(df2, value_vars = num)
n1 = sns.FacetGrid (nd, col='variable', col_wrap=4, sharex=False, sharey = False)
n1 = n1.map(sns.distplot, 'value')
n1
```

{% include figure image_path="/assets/190426/dis-plot.png" alt="dis-plot" %}

## Modeling
After exploring your data and subsetting the data into predictor variables (X) and outcome measures(y), we are two steps close to building predictive models. 

### ✳︎ Transformation (Standardization/ Normalization)
[This Medium blog](https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029) explains the reason why normalization is required when features have different ranges.

> The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values.

```python
from sklearn.preprocessing import StandardScaler

# Scale your data
scaler = StandardScaler()
scaler.fit(X) 
X_scaled = pd.DataFrame(scaler.transform(X),columns = X.columns)
```
### ✳︎ Data partition
To avoid overfitting, the data partition is the usual basic wayway. Here you can set a seed to regenerate the same split of the data.
```python
from sklearn.model_selection import train_test_split

X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X_scaled, Y, test_size=0.2, random_state=42)
```
### ✳︎ Modeling
####  Build models using training data and evaulate models using validation data
```python
# Test options and evaluation metric
seed = 7
scoring = 'accuracy'

# Spot Check Algorithms

models = []
models.append(('LR', LogisticRegression(solver='lbfgs', multi_class='ovr')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
#evaluate each model in turn
results = []
names = []

for name, model in models:
	kfold = model_selection.KFold(n_splits=10, random_state=seed)
	cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
	print(msg)


# Compare Algorithms
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()
```
####  Reevaluate models using test data

```python
# KNN: Make predictions on validation dataset
knn = KNeighborsClassifier()
knn.fit(X_train, Y_train)
predictions = knn.predict(X_validation)
print(accuracy_score(Y_validation, predictions))
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))
```
#### Predict/ classify using new data and model interpretation
After building the best model, you can then predict or classify the target variable by using new data. Besides, the most important variables can be identified by using the `permutation_importance`. I'll cover this topic in another post. Stay tuned!

{% include figure image_path="/assets/190426/minh-pham.jpg" alt="dog" %}
